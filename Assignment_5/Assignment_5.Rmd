---
title: "Assignment 5"
author: "Balaji Ravi Kumar"
date: "15/04/2022"
output: html_document
---
#Importing all the required libraries



```{r}
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
```

```{r}

#Importing the dataset of cereals
Cereals_Data<- read.csv("Cereals.csv")
Data_cereals <- data.frame(Cereals_Data[,4:16])
```

```{r}

#Processing the missing values 
Data_cereals <- na.omit(Data_cereals)

#Data Normalization
Cereals_Normalization <- scale(Data_cereals)
```

```{r}
#To normalize measurements applying the hierarchical clustering to the data using Euclidean distance.

Distance <- dist(Cereals_Normalization, method = "euclidean")
hierarchial.clust_complete <- hclust(Distance, method = "complete")

#Plotting the dendogram

plot(hierarchial.clust_complete, cex = 0.7, hang = -1)
```

```{r}
#Using agnes function to perfrom clustering with single, complete , average and Ward linkages.

hierarchial.clust_single <- agnes(Cereals_Normalization, method = "single")
hierarchial.clust_complete <- agnes(Cereals_Normalization, method = "complete")
hierarchial.clust_average <- agnes(Cereals_Normalization, method = "average")
hierarchial.clust_ward <- agnes(Cereals_Normalization, method = "ward")

#Comparing Single Linkage vs Complete Linkage vs Average Linkage vs Ward
print(hierarchial.clust_single$ac)
print(hierarchial.clust_complete$ac)
print(hierarchial.clust_average$ac)
print(hierarchial.clust_ward$ac)
```
# WARD method has the highest value of 0.9046042,  so we will consider it. 

#2. Choosing the clusters:
```{r}
pltree(hierarchial.clust_ward, cex = 0.7, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(hierarchial.clust_ward, k = 5, border = 1:4)
Cluster1 <- cutree(hierarchial.clust_ward, k=5)
dataframe2 <- as.data.frame(cbind(Cereals_Normalization,Cluster1))
```
#We will choose 5 clusters after observing the distance.
```{r}
#Creating Partitions
set.seed(64060)
Partition1 <- Data_cereals[1:50,]
Partition2 <- Data_cereals[51:74,]
```

```{r}
#Performing Hierarchial Clustering,consedering k = 5.
AG_single <- agnes(scale(Partition1), method = "single")
AG_complete <- agnes(scale(Partition1), method = "complete")
AG_average <- agnes(scale(Partition1), method = "average")
AG_ward <- agnes(scale(Partition1), method = "ward")
cbind(single=AG_single$ac , complete=AG_complete$ac , average= AG_average$ac , ward= AG_ward$ac)
pltree(AG_ward, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(AG_ward, k = 5, border = 1:4)
cut_2 <- cutree(AG_ward, k = 5)
```

```{r}
#Calculating the centeroids.
result <- as.data.frame(cbind(Partition1, cut_2))
result[result$cut_2==1,]

centroid_1 <- colMeans(result[result$cut_2==1,])
result[result$cut_2==2,]

centroid_2 <- colMeans(result[result$cut_2==2,])
result[result$cut_2==3,]

centroid_3 <- colMeans(result[result$cut_2==3,])
result[result$cut_2==4,]

centroid_4 <- colMeans(result[result$cut_2==4,])
centroids <- rbind(centroid_1, centroid_2, centroid_3, centroid_4)
x2 <- as.data.frame(rbind(centroids[,-14], Partition2))
```

```{r}
#Calculating the Distance
Distance_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Distance_1)
dataframe1 <- data.frame(data=seq(1,nrow(Partition2),1), Clusters = rep(0,nrow(Partition2)))
for(i in 1:nrow(Partition2)) 
{dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1
cbind(dataframe2$Cluster1[51:74], dataframe1$Clusters)
table(dataframe2$Cluster1[51:74] == dataframe1$Clusters)
```
#We can say that the model is partially stable as we are getting 12 FALSE and 12 TRUE 

#3) The elementary public schools would like to choose a set of Cereals_Data
# to include in their daily cafeterias. Every day a different cereal is offered,
# but all Cereals_Data should support a healthy diet. For this goal,
# you are requested to find a cluster of “healthy Cereals_Data.” 
```{r}

#Clustering Healthy Cereals_Data.
Healthy_Cereals <- Cereals_Data
Healthy_Cereals_new <- na.omit(Healthy_Cereals)
HealthyClust <- cbind(Healthy_Cereals_new, Cluster1)
HealthyClust[HealthyClust$Cluster1==1,]
HealthyClust[HealthyClust$Cluster1==2,]
HealthyClust[HealthyClust$Cluster1==3,]
HealthyClust[HealthyClust$Cluster1==4,]
```

```{r}
#Mean ratings to determine the best cluster.
mean(HealthyClust[HealthyClust$Cluster1==1,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==2,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==3,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==4,"rating"])
```
#Based on the above analysis, we can observe that the cluster 1 is recommended for elementary public schools as it has highest ratings of 73.84 among all the clusters.